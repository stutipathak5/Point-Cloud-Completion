optimizer : {
  type: AdamW,
  kwargs: {
  lr : 0.0008,
  weight_decay : 0.0005
}}

scheduler: {
  type: LambdaLR,
  kwargs: {
  decay_step: 21,
  lr_decay: 0.9,
  lowest_decay: 0.02  # min lr = lowest_decay * lr
}}

bnmscheduler: {
  type: Lambda,
  kwargs: {
  decay_step: 21,
  bn_decay: 0.5,
  bn_momentum: 0.9,
  lowest_decay: 0.01
}}

dataset : {
  train : { _base_: cfgs/dataset_configs/PCN.yaml, 
            others: {subset: 'train'}},
  val : { _base_: cfgs/dataset_configs/PCN.yaml, 
            others: {subset: 'test'}},
  test : { _base_: cfgs/dataset_configs/PCN.yaml, 
            others: {subset: 'test'}}}
model : {
  NAME: DictFold, num_pred: 16384, num_seeds: 1024, dim_feat: 512, num_dicts: 128, upscales: [1,4,4]}

loss : {
  sparse_loss_weight: 1.0,
  dense_loss_weight: 1.0,
  dz_weight: 1.0,
  orth_weight: 1.0,
}
total_bs : 32
step_per_update : 1
max_epoch : 400

consider_metric: CDL1
