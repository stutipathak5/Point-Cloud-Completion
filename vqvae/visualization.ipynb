{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model, parameters, performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spathak\\AppData\\Local\\Temp\\ipykernel_892\\3632031467.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(path + model_filename)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from models.vqvae import VQVAE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\"\"\"\n",
    "Utility functions\n",
    "\"\"\"\n",
    "\n",
    "def load_model(model_filename):\n",
    "    path = os.getcwd() + '/results/'\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        data = torch.load(path + model_filename)\n",
    "    else:\n",
    "        data = torch.load(path+model_filename,map_location=lambda storage, loc: storage)\n",
    "    \n",
    "    params = data[\"hyperparameters\"]\n",
    "    \n",
    "    model = VQVAE(params['n_hiddens'], params['n_residual_hiddens'],\n",
    "                  params['n_residual_layers'], params['n_embeddings'], \n",
    "                  params['embedding_dim'], params['beta']).to(device)\n",
    "\n",
    "    model.load_state_dict(data['model'])\n",
    "    \n",
    "    return model, data\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def plot_metrics(data):\n",
    "    results = data[\"results\"]\n",
    "    recon_errors = savgol_filter(results[\"recon_errors\"], 19, 5)\n",
    "    perplexities = savgol_filter(results[\"perplexities\"], 19, 5)\n",
    "    loss_vals = savgol_filter(results[\"loss_vals\"], 19, 5)\n",
    "\n",
    "\n",
    "    f = plt.figure(figsize=(16,4))\n",
    "    ax = f.add_subplot(1,3,2)\n",
    "    ax.plot(recon_errors)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title('Reconstruction Error')\n",
    "    ax.set_xlabel('iteration')\n",
    "\n",
    "    ax = f.add_subplot(1,3,3)\n",
    "    ax.plot(perplexities)\n",
    "    ax.set_title('Average codebook usage (perplexity).')\n",
    "    ax.set_xlabel('iteration')\n",
    "\n",
    "    ax = f.add_subplot(1,3,1)\n",
    "    ax.plot(loss_vals)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title('Overall Loss')\n",
    "    ax.set_xlabel('iteration')\n",
    "    \n",
    "def display_image_grid(x):\n",
    "    x = make_grid(x.cpu().detach()+0.5)\n",
    "    x = x.numpy()\n",
    "    fig = plt.imshow(np.transpose(x, (1,2,0)), interpolation='nearest')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "def reconstruct(data_loader,model):\n",
    "    (x, _) = next(iter(data_loader))\n",
    "    x = x.to(device)\n",
    "    vq_encoder_output = model.pre_quantization_conv(model.encoder(x))\n",
    "    _, z_q, _, _,e_indices = model.vector_quantization(vq_encoder_output)\n",
    "    \n",
    "    x_recon = model.decoder(z_q)\n",
    "    return x,x_recon, z_q,e_indices\n",
    "\n",
    "\"\"\"\n",
    "End of utilities\n",
    "\"\"\"\n",
    "\n",
    "model_filename = 'vqvae_data_static_block_data_ne512_de64_1000.pth'\n",
    "\n",
    "model,vqvae_data = load_model(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading block data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\spathak\\\\Downloads\\\\PCC\\\\vqvae/data/randact_traj_length_100_n_trials_1000_n_contexts_1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m training_data, validation_data, training_loader, validation_loader, x_train_var \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data_and_data_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBLOCK\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\spathak\\Downloads\\PCC\\vqvae\\utils.py:82\u001b[0m, in \u001b[0;36mload_data_and_data_loaders\u001b[1;34m(dataset, batch_size)\u001b[0m\n\u001b[0;32m     79\u001b[0m     x_train_var \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvar(training_data\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLOCK\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 82\u001b[0m     training_data, validation_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     training_loader, validation_loader \u001b[38;5;241m=\u001b[39m data_loaders(\n\u001b[0;32m     84\u001b[0m         training_data, validation_data, batch_size)\n\u001b[0;32m     86\u001b[0m     x_train_var \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvar(training_data\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\spathak\\Downloads\\PCC\\vqvae\\utils.py:33\u001b[0m, in \u001b[0;36mload_block\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m data_folder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m     30\u001b[0m data_file_path \u001b[38;5;241m=\u001b[39m data_folder_path \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/randact_traj_length_100_n_trials_1000_n_contexts_1.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 33\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mBlockDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNormalize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                             \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m val \u001b[38;5;241m=\u001b[39m BlockDataset(data_file_path, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m                    transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     42\u001b[0m                        transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     43\u001b[0m                        transforms\u001b[38;5;241m.\u001b[39mNormalize(\n\u001b[0;32m     44\u001b[0m                            (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))\n\u001b[0;32m     45\u001b[0m                    ]))\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train, val\n",
      "File \u001b[1;32mc:\\Users\\spathak\\Downloads\\PCC\\vqvae\\datasets\\block.py:14\u001b[0m, in \u001b[0;36mBlockDataset.__init__\u001b[1;34m(self, file_path, train, transform)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading block data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone loading block data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([cv2\u001b[38;5;241m.\u001b[39mresize(x[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][:, :, :\u001b[38;5;241m3\u001b[39m], dsize\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m), interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_CUBIC) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data])\n",
      "File \u001b[1;32mc:\\Users\\spathak\\Anaconda3\\envs\\pcc\\lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\spathak\\\\Downloads\\\\PCC\\\\vqvae/data/randact_traj_length_100_n_trials_1000_n_contexts_1.npy'"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "training_data, validation_data, training_loader, validation_loader, x_train_var = utils.load_data_and_data_loaders('BLOCK', 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruct validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val,x_val_recon,z_q,e_indices = reconstruct(validation_loader,model)\n",
    "print(x_val.shape)\n",
    "display_image_grid(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_grid(x_val_recon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothed Loss and Perplexity Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(vqvae_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from latent space $z$\n",
    "\n",
    "Sampling from VQ VAEs is more subtle than normal sampling schemes. To elucidate this point we'll present three sampling schemes:\n",
    "\n",
    "1. Uniform sampling\n",
    "2. Categorical sampling from a histogram \n",
    "3. Sampling with an autoregressive PixelCNN\n",
    "\n",
    "Sampling scheme (1) produces scrambled results, because only a small percentage of all possible representations are actually utilized. If you sample uniformly, you will most likely get results outside of the data distribution, which is why they appear random.\n",
    "\n",
    "Sampling scheme (2) collects a histogram of representations and samples from the histogram. This works but is also incorrect because it limits us to only representations seen during construction of the histogram. Note that if $N_{\\text{histogram}} \\rightarrow \\infty $ then this scheme will work, but since we can't actually do this, we want a better way to approximate the disribution of our representation space.\n",
    "\n",
    "Scheme (3) is more complex but provides a principled way of sampling from the latent space. We train a Gated PixelCNN to approximate the distribution of the latent space. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = vqvae_data['hyperparameters']\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform sampling of latent space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(e_indices):\n",
    "    min_encodings = torch.zeros(e_indices.shape[0], params['n_embeddings']).to(device)\n",
    "    min_encodings.scatter_(1, e_indices, 1)\n",
    "    e_weights = model.vector_quantization.embedding.weight\n",
    "    z_q = torch.matmul(min_encodings, e_weights).view((params[\"batch_size\"],8,8,params[\"embedding_dim\"])) \n",
    "    z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    " \n",
    "    x_recon = model.decoder(z_q)\n",
    "    return x_recon, z_q,e_indices\n",
    "    \n",
    "\n",
    "def uniform_samples(model):\n",
    " \n",
    "    rand = np.random.randint(params['n_embeddings'], size=(2048, 1))\n",
    "    min_encoding_indices = torch.tensor(rand).long().to(device)\n",
    "    x_recon, z_q,e_indices = generate_samples(min_encoding_indices)\n",
    "    \n",
    "    print(min_encoding_indices.shape)\n",
    "    return x_recon, z_q,e_indices\n",
    "\n",
    "x_val_recon,z_q,e_indices = uniform_samples(model)\n",
    "\n",
    "display_image_grid(x_val_recon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorial Distribution Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "    \n",
    "def encode_observations():\n",
    "    all_e_indices = []\n",
    "    for i in range(N):\n",
    "        _,_,_,e_indices = reconstruct(validation_loader,model)\n",
    "        all_e_indices.append(e_indices)\n",
    "\n",
    "    return torch.cat(all_e_indices)\n",
    "\n",
    "e_indices = encode_observations()\n",
    "\n",
    "def count_representations():\n",
    "    d = {}\n",
    "    for i in range(32*N):\n",
    "        k = e_indices[64*i:64*i+64].squeeze().cpu().detach().numpy()\n",
    "        k = [str(j)+'-' for j in k]\n",
    "        k = ''.join(k)\n",
    "\n",
    "        if k not in d:\n",
    "            d[k] = 1\n",
    "        else:\n",
    "            d[k]+=1\n",
    "    \n",
    "    return d\n",
    "\n",
    "hist = count_representations()\n",
    "if '' in hist: del hist['']\n",
    "\n",
    "print('Total representations used:',len(hist.keys()))\n",
    "def sample_histogram(hist):\n",
    "    keys, vals = np.array(list(hist.keys())),np.array(list(hist.values()))\n",
    "    probs = np.array(vals)/sum(vals)\n",
    "    samples = np.random.choice(keys,params['batch_size'],p=probs,replace=True)\n",
    "    \n",
    "    samples = np.array([np.array([int(y) for y in x.split('-')[:-1]]) for x in samples])\n",
    "    \n",
    "    return samples\n",
    "    \n",
    "samples = sample_histogram(hist)\n",
    "\n",
    "def histogram_samples(model):\n",
    "    min_encoding_indices = torch.tensor(samples).reshape(-1,1).long().to(device)\n",
    "    x_recon, z_q,e_indices = generate_samples(min_encoding_indices)\n",
    "    \n",
    "    return x_recon, z_q,e_indices\n",
    "\n",
    "x_hist,_,_ = histogram_samples(model)\n",
    "\n",
    "display_image_grid(x_hist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most common representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_sample_index = np.argmax(list(hist.values()))\n",
    "most_common_z = np.array([int(y) for y in list(hist.keys())[most_common_sample_index].split('-')[:-1]])\n",
    "\n",
    "\n",
    "def most_common_samples(model):\n",
    "    \n",
    "    samples = np.array(list(most_common_z)*32).reshape(-1,1)\n",
    "    print(samples.shape)\n",
    "    min_encoding_indices = torch.tensor(samples).reshape(-1,1).long().to(device)\n",
    "    x_recon, z_q,e_indices = generate_samples(min_encoding_indices)\n",
    "    \n",
    "    return x_recon, z_q,e_indices\n",
    "\n",
    "\n",
    "x_val_recon,z_q,e_indices = most_common_samples(model)\n",
    "\n",
    "display_image_grid(x_val_recon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruct from PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_folder_path = os.getcwd() \n",
    "data_file_path = data_folder_path + '/data/latent_samples.npy'\n",
    "\n",
    "samples = np.load(data_file_path,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_from_pixelcnn(model,samples):\n",
    "    \n",
    "\n",
    "    min_encoding_indices = torch.tensor(samples).reshape(-1,1).long().to(device)\n",
    "    x_recon, z_q,e_indices = generate_samples(min_encoding_indices)\n",
    "    \n",
    "    return x_recon, z_q,e_indices\n",
    "\n",
    "\n",
    "x_val_recon,z_q,e_indices = reconstruct_from_pixelcnn(model,samples)\n",
    "\n",
    "display_image_grid(x_val_recon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. load all data\n",
    "2. compute COM for all data\n",
    "3. get representation indices for all data and hash them\n",
    "4. organize all data by index for steps 1-3\n",
    "5. build unique color scheme for all used representations\n",
    "6. iterate through hash values and color the COM pixel with hash values\n",
    "\n",
    "display resulting image\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "def encode_data(data,model):\n",
    "    x = data.data # assumes youre using Pytorch formatted dataset\n",
    "    x = torch.tensor(x).float().to(device)\n",
    "    x = x.permute(0,3,1,2).contiguous()\n",
    "    x = x.to(device)\n",
    "    vq_encoder_output = model.pre_quantization_conv(model.encoder(x))\n",
    "    _, z_q, _, _,e_indices = model.vector_quantization(vq_encoder_output)\n",
    "    \n",
    "    x_recon = model.decoder(z_q)\n",
    "    return x,x_recon, z_q,e_indices\n",
    "\n",
    "def count_and_hash_representations(e_indices,prune=2):\n",
    "    x_hashes = []\n",
    "    d = {}\n",
    "    n = int(len(e_indices)/64)\n",
    "    for i in range(n):\n",
    "        k = e_indices[64*i:64*i+64].squeeze().cpu().detach().numpy()\n",
    "        hash_ = hash(tuple(k))\n",
    "\n",
    "        if hash_ not in d:\n",
    "            d[hash_] = 1\n",
    "        else:\n",
    "            d[hash_]+=1\n",
    "            \n",
    "        x_hashes.append(hash_)\n",
    "            \n",
    "    # prune hash table\n",
    "    d = dict((k, v) for k, v in d.items() if v >= prune)\n",
    "    \n",
    "    return d,x_hashes\n",
    "\n",
    "\n",
    "def create_color_template(n):\n",
    "    num_shades = n\n",
    "    if n < 1000:\n",
    "        sns.palplot(sns.husl_palette(num_shades))\n",
    "        color_list = sns.husl_palette(num_shades)\n",
    "    else:\n",
    "        sns.palplot(sns.cubehelix_palette(num_shades))\n",
    "        color_list = sns.cubehelix_palette(num_shades)\n",
    "        \n",
    "\n",
    "    rgb_list = []\n",
    "    for color in color_list:\n",
    "        rgb = []\n",
    "        for value in color:\n",
    "            value *= 255\n",
    "            rgb.append(int(value))\n",
    "        rgb_list.append(np.array(rgb).astype(int))\n",
    "\n",
    "    return rgb_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "PRUNE = 0\n",
    "N_TOP_REPS = 500\n",
    "\n",
    "# get data and discrete indices\n",
    "data,_,_,e_indices = encode_data(training_data,model)\n",
    "\n",
    "# create a hash table with most common representations\n",
    "d, data_hashes = count_and_hash_representations(e_indices,PRUNE)\n",
    "\n",
    "d = dict(Counter(d).most_common(N_TOP_REPS))\n",
    "\n",
    "# count num of reps\n",
    "n = len(d.keys())\n",
    "print(n)\n",
    "colors = create_color_template(n)\n",
    "color_hash_table = {k:v for k,v in zip(d.keys(),colors)}\n",
    "\n",
    "data = data.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "color_img = np.zeros((32,32,3))\n",
    "\n",
    "colored_data = data.copy()\n",
    "\n",
    "colored_idxs = []\n",
    "\n",
    "colored_img_dict = {}\n",
    "\n",
    "for k,(x,rep) in enumerate(zip(data,data_hashes)):\n",
    "    x = np.transpose(x,(1,2,0))\n",
    "    \n",
    "    block_ij = np.argwhere(x[:,:,1]>100)\n",
    "    \n",
    "    x_min = np.min(block_ij,0)\n",
    "    x_max = np.max(block_ij,0)\n",
    "    \n",
    "    i,j = (x_min + x_max)//2\n",
    "    \n",
    "    if rep not in color_hash_table:\n",
    "        color = np.array([255,255,255])\n",
    "        for idx in block_ij:\n",
    "            row,col = idx\n",
    "            colored_data[k,0,row,col] = color[0]\n",
    "            colored_data[k,1,row,col] = color[1]\n",
    "            colored_data[k,2,row,col] = color[2]\n",
    "    else:\n",
    "        colored_idxs.append(k)\n",
    "        \n",
    "            \n",
    "        color = np.array(color_hash_table[rep])\n",
    "        color_img[i,j] = color\n",
    "        for idx in block_ij:\n",
    "            row,col = idx\n",
    "            colored_data[k,0,row,col] = color[0]\n",
    "            colored_data[k,1,row,col] = color[1]\n",
    "            colored_data[k,2,row,col] = color[2]\n",
    "            color_img[row,col]=color\n",
    "            \n",
    "        if rep not in colored_img_dict:\n",
    "            colored_img_dict[rep] = [colored_data[k,:,:,:]]\n",
    "        else:\n",
    "            colored_img_dict[rep].append(colored_data[k,:,:,:])\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.transpose(data[8],(1,2,0))\n",
    "x.shape\n",
    "\n",
    "imgplot = plt.imshow(color_img.astype(int))\n",
    "#imgplot.axes.get_xaxis().set_visible(False)\n",
    "#imgplot.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_list = []\n",
    "\n",
    "for rep_imgs in list(colored_img_dict.values()):\n",
    "    image_list.append(torch.tensor(np.array(rep_imgs)[:4]).int())\n",
    "\n",
    "x = torch.cat(image_list[:10])\n",
    "for _ in range(2):\n",
    "    display_image_grid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How can we build a graph from a color image?\n",
    "\n",
    "Step 1: set up\n",
    "\n",
    "- Create an index to representation map (e.g. an array of all the representations) of length N\n",
    "- Create a color to index hash table\n",
    "- Create new image representation that takes color image and replaces all RGB colors with indices\n",
    "- Create a N X N matrix of zeros\n",
    "\n",
    "Step 2: graph construction\n",
    "\n",
    "Scan the color image twice - horizontally and vertically\n",
    "\n",
    "1. start at origin pixel (0,0), mark its color\n",
    "While pixels not exhausted\n",
    "2. move one step\n",
    "3. check if color at this step is the same as previous color\n",
    "4. if color is same, skip step 5\n",
    "5. if color is different, add 1 to matrix[i][j] and matrix[j][i] where i is previous color and j is current color\n",
    "6. mark current pixel color and move on to next pixel\n",
    "\n",
    "do this twice for vertical and horizontal pixels\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Create an index to representation map\n",
    "# add black (None representation)\n",
    "index_rep_map = [np.array([0,0,0])]\n",
    "# add representations\n",
    "index_rep_map+=list(color_hash_table.values())\n",
    "index_rep_map = np.array(index_rep_map)\n",
    "\n",
    "# Create a color to index hash table\n",
    "color_index_hash_table = {str(v):i for i,v in enumerate(index_rep_map)}\n",
    "# Create new image representation that takes color image and replaces all RGB colors with indices\n",
    "count = 0\n",
    "# iterate over colors\n",
    "index_img = np.zeros((32,32))\n",
    "\n",
    "color_img_int = color_img.astype(int)\n",
    "for i in range(color_img_int.shape[0]):\n",
    "    for j in range(color_img_int.shape[1]):\n",
    "        color_string = str(color_img_int[i,j,:])\n",
    "        index_img[i,j] = color_index_hash_table[color_string]\n",
    "index_img = index_img.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 graph init\n",
    "\n",
    "# Create a N X N matrix of zeros\n",
    "n = len(index_rep_map)\n",
    "graph = np.zeros((n,n)).astype(int)\n",
    "\n",
    "# Step 2\n",
    "\n",
    "# horizontal and vertical scans\n",
    "m = index_img.shape[0]\n",
    "horizontal_mark = None\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(m):\n",
    "        # horizontal scan\n",
    "        # make sure j-1 > -1\n",
    "        if j>0:\n",
    "            current_color = index_img[i][j]\n",
    "            previous_color = index_img[i][j-1]\n",
    "            if current_color != previous_color:\n",
    "                graph[current_color][previous_color]+=1\n",
    "                graph[previous_color][current_color]+=1\n",
    "        # vertical scan\n",
    "        # make sure i-1<-1\n",
    "        if i>0:\n",
    "            current_color = index_img[i][j]\n",
    "            previous_color = index_img[i-1][j]\n",
    "            if current_color != previous_color:\n",
    "                graph[current_color][previous_color]+=1\n",
    "                graph[previous_color][current_color]+=1\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_graph = graph.copy()\n",
    "binary_graph[graph>0]=1\n",
    "binary_graph_of_reps_only = binary_graph[1:,1:]\n",
    "binary_graph_of_reps_only.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "shortest path algorithm - BFS\n",
    "\n",
    "graph representation: adjacency dict\n",
    "\"\"\"\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def adjacency_matrix_to_dict(matrix):\n",
    "    n = matrix.shape[0]\n",
    "    g_dict = {k:set([]) for k in range(n)}\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if matrix[i][j]:\n",
    "                g_dict[i].add(j)\n",
    "    g_dict = {k:list(v) for k,v in g_dict.items()}\n",
    "    return g_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_list = adjacency_matrix_to_dict(binary_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFS(G,start,end):\n",
    "    #assert start in G and end in G, 'start (or) end node not in Graph'\n",
    "    #assert G[start] and G[end], 'start (or) end nodes are disjoint'\n",
    "    q = deque([])\n",
    "    q.append([start])\n",
    "    \n",
    "    visited =set([start])\n",
    "    path = []\n",
    "    \n",
    "    while q:\n",
    "        \n",
    "        path = q.popleft()\n",
    "        node = path[-1]\n",
    "        if node == end:\n",
    "            return path\n",
    "        else:\n",
    "            for adjacent in G.get(node,[node]):\n",
    "                new_path = list(path)\n",
    "                new_path.append(adjacent)\n",
    "                q.append(new_path)\n",
    " \n",
    "    return []\n",
    "\n",
    "BFS(adjacency_list,11,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ASIDE: what's a better way to construct a graph?\n",
    "\n",
    "Currently, using the color image as ground truth is a bad a idea because it is a lossy representation of actual\n",
    "connections.\n",
    "\n",
    "The best way to build a graph is to run VQ VAE on data and keep track of transitions \n",
    "\n",
    "e.g. each time z_i -> z_j and i != j, hash the representation and +=1 in some monster adjacency dictionary\n",
    "to keep the representations fixed, you can pick to N reps and then only count if the transition is in one of those \n",
    "reps\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run local policy with a reward specified from global policy\n",
    "\n",
    "Given start observation o_1 and end observation o_2, construct plan with graph\n",
    "\n",
    "E.g.\n",
    "suppose o_1 -> z_7 and o_2 -> z_9 construct graph z_7 -> z_0 -> z_1 -> z_9\n",
    "and then train goal conditioned policy that needs to solve curriculum\n",
    "pi(a|o,z_goal) where z_goal = z_0 then z_1 and then z_9 depending on which current z_j you're in\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "What kind of policy should we run?\n",
    "\n",
    "Off-policy:\n",
    "\n",
    "In the off-policy case, we can collect a bunch of data (i.e. the space will be explored), we can then \n",
    "- collect a replay buffer of observations\n",
    "- then train a VQ VAE\n",
    "- then create a replay buffer with latents \n",
    "- then learn a goal conditioned policy\n",
    "\n",
    "How do we learn goal conditioned policies? Try the HER thing\n",
    "\n",
    "HER thing: input concatenated state || goal and train policy that way\n",
    "\n",
    "how does it work with representation learning?\n",
    "\n",
    "- after each trajectory (z_1,_z_1,z_17,z_5,z_5, etc) relabel as though agent was trying to achieve\n",
    "some latent state in the trajectory (e.g. z_j) and store transitions (z_i || z_j) in the replay buffer\n",
    "\n",
    "(Notes:\n",
    "- by the way you can go through ALL the representations achieved\n",
    "- maybe best to just reward the final latent observation so that it is always forward thinking\n",
    ")\n",
    "- after this policy is trained it should be able to arrive at any latent goal state z_i\n",
    "\n",
    "how do we treat representations - i.e. use entire matrix or an encoding?\n",
    "\n",
    "\n",
    "PLAN:\n",
    "\n",
    "Try with Block world. \n",
    "- Train VQ VAE on random block world\n",
    "- Train HER on latent representation \n",
    "    - flatten the 8x8 latent image into a 64 dim vector\n",
    "- Optional: provide .1 points for moving to different representation than current one, and 1 point for moving to\n",
    "  HER representation \n",
    "- use DDPG or TD3 variant (continuous actions)\n",
    "- At test time make a plan z_1 - z_2 - z_N and have agent reach each goal individually\n",
    "- Retrain VQ VAE periodically, and reconstruct graph\n",
    "\n",
    "options for reward relabling\n",
    "\n",
    "option 1: teach agent to reach any goal\n",
    "1. train time - normal HER\n",
    "2. test time - make a plan z_1 -> z_2 ... -> z_g and have policy follow plan\n",
    "\n",
    "option 2: teach agent to reach goal sequence\n",
    "1. train time - pick random future goal like in HER, find graph to said goal in path and reward path states\n",
    "2. test time - same as (2) previously\n",
    "\n",
    "option 1 is simpler let's do that\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FUTURE:\n",
    "\n",
    "train in robotic simulators e.g. fetch reach\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "how do we make a better / cleaner graph?\n",
    "\n",
    "first, run a VQ VAE over the data and construct the graph from transitions NOT from the final color image\n",
    "\n",
    "we can choose N representations put them in set S and run the VQ VAE over the data\n",
    "then if the encoding is inside the set S, we keep it, if not we set it to the null encoding \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
